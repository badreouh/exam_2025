{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CACBFsndOCo"
      },
      "source": [
        "# Exercices"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Préliminaires**: Clone de votre repo et imports"
      ],
      "metadata": {
        "id": "hfkMtaHleKAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/badreouh/exam_2025.git\n",
        "! cp exam_2025/utils/utils_exercices.py .\n",
        "\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "xiD_cI-geJjI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99867f97-06ba-4f22-bb46-4134be9b8137"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'exam_2025' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clef personnelle pour la partie théorique**\n",
        "\n",
        "Dans la cellule suivante, choisir un entier entre 100 et 1000 (il doit être personnel). Cet entier servira de graine au générateur de nombres aléatoire a conserver pour tous les exercices.\n",
        "\n"
      ],
      "metadata": {
        "id": "J3ga_6BNc5DR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mySeed = 333"
      ],
      "metadata": {
        "id": "PrCTHM4od5UZ"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "TRWBLVpCWC06"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5RcggmAkJLV"
      },
      "source": [
        "\\\n",
        "\n",
        "**Exercice 1** *Une relation linéaire*\n",
        "\n",
        "La fonction *generate_dataset* fournit deux jeux de données (entraînement et test). Pour chaque jeu de données, la clef 'inputs' donne accès à un tableau numpy (numpy array) de prédicteurs empilés horizontalement : chaque ligne $i$ contient trois prédicteurs $x_i$, $y_i$ et $z_i$. La clef 'targets' renvoie le vecteur des cibles $t_i$. \\\n",
        "\n",
        "Les cibles sont liées aux prédicteurs par le modèle:\n",
        "$$ t = \\theta_0 + \\theta_1 x + \\theta_2 y + \\theta_3 z + \\epsilon$$ où $\\epsilon \\sim \\mathcal{N}(0,\\eta)$\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils_exercices import generate_dataset, Dataset1\n",
        "train_set, test_set = generate_dataset(mySeed)"
      ],
      "metadata": {
        "id": "gEQmgTI8my8i"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** Par quelle méthode simple peut-on estimer les coefficients $\\theta_k$ ? La mettre en oeuvre avec la librairie python de votre choix."
      ],
      "metadata": {
        "id": "q5XZTrXNk12K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#On peut utiliser la regression linéaire\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "# Extract inputs and targets from the training set\n",
        "X_train = train_set['inputs']\n",
        "y_train = train_set['targets']\n",
        "\n",
        "# Train a linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# coefficients\n",
        "theta0 = model.intercept_\n",
        "theta1, theta2, theta3 = model.coef_\n",
        "\n",
        "# the results\n",
        "print(f\"Estimated coefficients:\")\n",
        "print(f\"θ0 (Intercept): {theta0}\")\n",
        "print(f\"θ1 (x): {theta1}\")\n",
        "print(f\"θ2 (y): {theta2}\")\n",
        "print(f\"θ3 (z): {theta3}\")\n"
      ],
      "metadata": {
        "id": "HITtUqHhFMkn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ced4c5e5-fe6d-42dd-f2db-6c44cb91d3a1"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated coefficients:\n",
            "θ0 (Intercept): 16.558024510983287\n",
            "θ1 (x): 3.2729838244521834\n",
            "θ2 (y): 3.294835451763797\n",
            "θ3 (z): 6.687036331785982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MXGXg8tlPULY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Dans les cellules suivantes, on se propose d'estimer les $\\theta_k$ grâce à un réseau de neurones entraîné par SGD. Quelle architecture s'y prête ? Justifier en termes d'expressivité et de performances en généralisation puis la coder dans la cellule suivante."
      ],
      "metadata": {
        "id": "CH_Z5ZEIlQPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset et dataloader :\n",
        "dataset = Dataset1(train_set['inputs'], train_set['targets'])\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=100, shuffle=True)\n",
        "\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.fc = nn.Linear(3, 1)  # Linear layer for 3 inputs (x, y, z) and 1 output (t)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)  # Return the linear combination"
      ],
      "metadata": {
        "id": "PPx543blnxdb"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Entraîner cette architecture à la tâche de régression définie par les entrées et sorties du jeu d'entraînement (compléter la cellule ci-dessous)."
      ],
      "metadata": {
        "id": "g6BSTBitpGBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss, and optimizer\n",
        "mySimpleNet = SimpleNet()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(mySimpleNet.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 500\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0.0  # Track total loss for this epoch\n",
        "    for batch_inputs, batch_targets in dataloader:\n",
        "        optimizer.zero_grad()  # Reset gradients to zero\n",
        "\n",
        "        # Forward pass: compute predictions\n",
        "        outputs = mySimpleNet(batch_inputs)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(outputs.squeeze(), batch_targets)\n",
        "\n",
        "        # Backward pass: compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate loss for reporting\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Print the epoch loss\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}\")"
      ],
      "metadata": {
        "id": "Wjfa2Z4RoPO-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d18ffb24-448d-4d4b-f5a6-4a6034ab3d5f"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/500], Loss: 3500.9886\n",
            "Epoch [2/500], Loss: 2085.3168\n",
            "Epoch [3/500], Loss: 1254.0649\n",
            "Epoch [4/500], Loss: 764.5841\n",
            "Epoch [5/500], Loss: 476.0548\n",
            "Epoch [6/500], Loss: 305.6546\n",
            "Epoch [7/500], Loss: 204.4943\n",
            "Epoch [8/500], Loss: 144.1420\n",
            "Epoch [9/500], Loss: 107.9004\n",
            "Epoch [10/500], Loss: 85.9064\n",
            "Epoch [11/500], Loss: 72.3529\n",
            "Epoch [12/500], Loss: 63.8975\n",
            "Epoch [13/500], Loss: 58.4909\n",
            "Epoch [14/500], Loss: 54.9151\n",
            "Epoch [15/500], Loss: 52.4693\n",
            "Epoch [16/500], Loss: 50.7157\n",
            "Epoch [17/500], Loss: 49.4094\n",
            "Epoch [18/500], Loss: 48.3847\n",
            "Epoch [19/500], Loss: 47.5643\n",
            "Epoch [20/500], Loss: 46.8610\n",
            "Epoch [21/500], Loss: 46.2617\n",
            "Epoch [22/500], Loss: 45.7477\n",
            "Epoch [23/500], Loss: 45.2759\n",
            "Epoch [24/500], Loss: 44.8481\n",
            "Epoch [25/500], Loss: 44.4586\n",
            "Epoch [26/500], Loss: 44.1075\n",
            "Epoch [27/500], Loss: 43.7912\n",
            "Epoch [28/500], Loss: 43.5048\n",
            "Epoch [29/500], Loss: 43.2220\n",
            "Epoch [30/500], Loss: 42.9886\n",
            "Epoch [31/500], Loss: 42.7563\n",
            "Epoch [32/500], Loss: 42.5404\n",
            "Epoch [33/500], Loss: 42.3366\n",
            "Epoch [34/500], Loss: 42.1597\n",
            "Epoch [35/500], Loss: 41.9952\n",
            "Epoch [36/500], Loss: 41.8442\n",
            "Epoch [37/500], Loss: 41.7162\n",
            "Epoch [38/500], Loss: 41.5797\n",
            "Epoch [39/500], Loss: 41.4600\n",
            "Epoch [40/500], Loss: 41.3556\n",
            "Epoch [41/500], Loss: 41.2718\n",
            "Epoch [42/500], Loss: 41.1542\n",
            "Epoch [43/500], Loss: 41.0818\n",
            "Epoch [44/500], Loss: 40.9956\n",
            "Epoch [45/500], Loss: 40.9216\n",
            "Epoch [46/500], Loss: 40.8566\n",
            "Epoch [47/500], Loss: 40.7976\n",
            "Epoch [48/500], Loss: 40.7509\n",
            "Epoch [49/500], Loss: 40.7035\n",
            "Epoch [50/500], Loss: 40.6517\n",
            "Epoch [51/500], Loss: 40.5973\n",
            "Epoch [52/500], Loss: 40.5613\n",
            "Epoch [53/500], Loss: 40.5134\n",
            "Epoch [54/500], Loss: 40.4867\n",
            "Epoch [55/500], Loss: 40.4603\n",
            "Epoch [56/500], Loss: 40.4352\n",
            "Epoch [57/500], Loss: 40.4068\n",
            "Epoch [58/500], Loss: 40.3734\n",
            "Epoch [59/500], Loss: 40.3512\n",
            "Epoch [60/500], Loss: 40.3413\n",
            "Epoch [61/500], Loss: 40.3153\n",
            "Epoch [62/500], Loss: 40.2928\n",
            "Epoch [63/500], Loss: 40.2647\n",
            "Epoch [64/500], Loss: 40.2602\n",
            "Epoch [65/500], Loss: 40.2479\n",
            "Epoch [66/500], Loss: 40.2445\n",
            "Epoch [67/500], Loss: 40.2228\n",
            "Epoch [68/500], Loss: 40.2147\n",
            "Epoch [69/500], Loss: 40.1988\n",
            "Epoch [70/500], Loss: 40.1937\n",
            "Epoch [71/500], Loss: 40.1866\n",
            "Epoch [72/500], Loss: 40.1658\n",
            "Epoch [73/500], Loss: 40.1712\n",
            "Epoch [74/500], Loss: 40.1588\n",
            "Epoch [75/500], Loss: 40.1645\n",
            "Epoch [76/500], Loss: 40.1492\n",
            "Epoch [77/500], Loss: 40.1538\n",
            "Epoch [78/500], Loss: 40.1340\n",
            "Epoch [79/500], Loss: 40.1444\n",
            "Epoch [80/500], Loss: 40.1290\n",
            "Epoch [81/500], Loss: 40.1235\n",
            "Epoch [82/500], Loss: 40.1409\n",
            "Epoch [83/500], Loss: 40.1219\n",
            "Epoch [84/500], Loss: 40.1154\n",
            "Epoch [85/500], Loss: 40.1112\n",
            "Epoch [86/500], Loss: 40.1144\n",
            "Epoch [87/500], Loss: 40.1194\n",
            "Epoch [88/500], Loss: 40.1106\n",
            "Epoch [89/500], Loss: 40.1110\n",
            "Epoch [90/500], Loss: 40.1259\n",
            "Epoch [91/500], Loss: 40.0991\n",
            "Epoch [92/500], Loss: 40.1072\n",
            "Epoch [93/500], Loss: 40.0985\n",
            "Epoch [94/500], Loss: 40.1073\n",
            "Epoch [95/500], Loss: 40.0956\n",
            "Epoch [96/500], Loss: 40.0892\n",
            "Epoch [97/500], Loss: 40.0909\n",
            "Epoch [98/500], Loss: 40.0906\n",
            "Epoch [99/500], Loss: 40.0925\n",
            "Epoch [100/500], Loss: 40.0945\n",
            "Epoch [101/500], Loss: 40.0919\n",
            "Epoch [102/500], Loss: 40.0894\n",
            "Epoch [103/500], Loss: 40.0820\n",
            "Epoch [104/500], Loss: 40.0904\n",
            "Epoch [105/500], Loss: 40.0818\n",
            "Epoch [106/500], Loss: 40.0873\n",
            "Epoch [107/500], Loss: 40.0912\n",
            "Epoch [108/500], Loss: 40.0870\n",
            "Epoch [109/500], Loss: 40.0998\n",
            "Epoch [110/500], Loss: 40.0864\n",
            "Epoch [111/500], Loss: 40.0754\n",
            "Epoch [112/500], Loss: 40.0813\n",
            "Epoch [113/500], Loss: 40.0853\n",
            "Epoch [114/500], Loss: 40.0784\n",
            "Epoch [115/500], Loss: 40.0855\n",
            "Epoch [116/500], Loss: 40.0871\n",
            "Epoch [117/500], Loss: 40.0976\n",
            "Epoch [118/500], Loss: 40.0805\n",
            "Epoch [119/500], Loss: 40.0824\n",
            "Epoch [120/500], Loss: 40.0757\n",
            "Epoch [121/500], Loss: 40.0979\n",
            "Epoch [122/500], Loss: 40.0757\n",
            "Epoch [123/500], Loss: 40.1002\n",
            "Epoch [124/500], Loss: 40.0933\n",
            "Epoch [125/500], Loss: 40.0885\n",
            "Epoch [126/500], Loss: 40.0856\n",
            "Epoch [127/500], Loss: 40.0941\n",
            "Epoch [128/500], Loss: 40.0940\n",
            "Epoch [129/500], Loss: 40.0834\n",
            "Epoch [130/500], Loss: 40.0804\n",
            "Epoch [131/500], Loss: 40.0808\n",
            "Epoch [132/500], Loss: 40.0843\n",
            "Epoch [133/500], Loss: 40.0893\n",
            "Epoch [134/500], Loss: 40.0900\n",
            "Epoch [135/500], Loss: 40.0808\n",
            "Epoch [136/500], Loss: 40.0897\n",
            "Epoch [137/500], Loss: 40.0812\n",
            "Epoch [138/500], Loss: 40.0882\n",
            "Epoch [139/500], Loss: 40.0882\n",
            "Epoch [140/500], Loss: 40.0915\n",
            "Epoch [141/500], Loss: 40.0797\n",
            "Epoch [142/500], Loss: 40.0802\n",
            "Epoch [143/500], Loss: 40.0851\n",
            "Epoch [144/500], Loss: 40.0840\n",
            "Epoch [145/500], Loss: 40.0849\n",
            "Epoch [146/500], Loss: 40.0812\n",
            "Epoch [147/500], Loss: 40.0832\n",
            "Epoch [148/500], Loss: 40.0897\n",
            "Epoch [149/500], Loss: 40.0951\n",
            "Epoch [150/500], Loss: 40.0856\n",
            "Epoch [151/500], Loss: 40.0798\n",
            "Epoch [152/500], Loss: 40.0803\n",
            "Epoch [153/500], Loss: 40.0853\n",
            "Epoch [154/500], Loss: 40.0869\n",
            "Epoch [155/500], Loss: 40.0806\n",
            "Epoch [156/500], Loss: 40.0820\n",
            "Epoch [157/500], Loss: 40.0857\n",
            "Epoch [158/500], Loss: 40.0987\n",
            "Epoch [159/500], Loss: 40.0774\n",
            "Epoch [160/500], Loss: 40.0819\n",
            "Epoch [161/500], Loss: 40.0932\n",
            "Epoch [162/500], Loss: 40.0796\n",
            "Epoch [163/500], Loss: 40.0780\n",
            "Epoch [164/500], Loss: 40.0957\n",
            "Epoch [165/500], Loss: 40.0906\n",
            "Epoch [166/500], Loss: 40.0933\n",
            "Epoch [167/500], Loss: 40.0786\n",
            "Epoch [168/500], Loss: 40.0805\n",
            "Epoch [169/500], Loss: 40.0826\n",
            "Epoch [170/500], Loss: 40.0861\n",
            "Epoch [171/500], Loss: 40.0909\n",
            "Epoch [172/500], Loss: 40.0816\n",
            "Epoch [173/500], Loss: 40.0875\n",
            "Epoch [174/500], Loss: 40.0919\n",
            "Epoch [175/500], Loss: 40.0785\n",
            "Epoch [176/500], Loss: 40.0865\n",
            "Epoch [177/500], Loss: 40.0843\n",
            "Epoch [178/500], Loss: 40.0844\n",
            "Epoch [179/500], Loss: 40.0864\n",
            "Epoch [180/500], Loss: 40.0873\n",
            "Epoch [181/500], Loss: 40.0765\n",
            "Epoch [182/500], Loss: 40.0935\n",
            "Epoch [183/500], Loss: 40.0857\n",
            "Epoch [184/500], Loss: 40.0763\n",
            "Epoch [185/500], Loss: 40.0763\n",
            "Epoch [186/500], Loss: 40.0802\n",
            "Epoch [187/500], Loss: 40.0800\n",
            "Epoch [188/500], Loss: 40.0951\n",
            "Epoch [189/500], Loss: 40.0868\n",
            "Epoch [190/500], Loss: 40.0816\n",
            "Epoch [191/500], Loss: 40.0777\n",
            "Epoch [192/500], Loss: 40.0836\n",
            "Epoch [193/500], Loss: 40.0890\n",
            "Epoch [194/500], Loss: 40.0908\n",
            "Epoch [195/500], Loss: 40.0971\n",
            "Epoch [196/500], Loss: 40.0838\n",
            "Epoch [197/500], Loss: 40.0808\n",
            "Epoch [198/500], Loss: 40.0930\n",
            "Epoch [199/500], Loss: 40.0831\n",
            "Epoch [200/500], Loss: 40.0772\n",
            "Epoch [201/500], Loss: 40.0763\n",
            "Epoch [202/500], Loss: 40.0940\n",
            "Epoch [203/500], Loss: 40.0848\n",
            "Epoch [204/500], Loss: 40.0957\n",
            "Epoch [205/500], Loss: 40.0812\n",
            "Epoch [206/500], Loss: 40.0919\n",
            "Epoch [207/500], Loss: 40.0814\n",
            "Epoch [208/500], Loss: 40.0925\n",
            "Epoch [209/500], Loss: 40.0872\n",
            "Epoch [210/500], Loss: 40.0840\n",
            "Epoch [211/500], Loss: 40.0903\n",
            "Epoch [212/500], Loss: 40.0954\n",
            "Epoch [213/500], Loss: 40.0750\n",
            "Epoch [214/500], Loss: 40.0910\n",
            "Epoch [215/500], Loss: 40.0825\n",
            "Epoch [216/500], Loss: 40.0913\n",
            "Epoch [217/500], Loss: 40.0831\n",
            "Epoch [218/500], Loss: 40.0832\n",
            "Epoch [219/500], Loss: 40.0836\n",
            "Epoch [220/500], Loss: 40.0906\n",
            "Epoch [221/500], Loss: 40.0834\n",
            "Epoch [222/500], Loss: 40.0772\n",
            "Epoch [223/500], Loss: 40.0783\n",
            "Epoch [224/500], Loss: 40.0920\n",
            "Epoch [225/500], Loss: 40.0844\n",
            "Epoch [226/500], Loss: 40.0885\n",
            "Epoch [227/500], Loss: 40.0890\n",
            "Epoch [228/500], Loss: 40.1037\n",
            "Epoch [229/500], Loss: 40.0773\n",
            "Epoch [230/500], Loss: 40.1037\n",
            "Epoch [231/500], Loss: 40.0796\n",
            "Epoch [232/500], Loss: 40.0890\n",
            "Epoch [233/500], Loss: 40.0804\n",
            "Epoch [234/500], Loss: 40.0868\n",
            "Epoch [235/500], Loss: 40.0819\n",
            "Epoch [236/500], Loss: 40.0897\n",
            "Epoch [237/500], Loss: 40.1028\n",
            "Epoch [238/500], Loss: 40.0801\n",
            "Epoch [239/500], Loss: 40.0913\n",
            "Epoch [240/500], Loss: 40.0870\n",
            "Epoch [241/500], Loss: 40.0990\n",
            "Epoch [242/500], Loss: 40.1007\n",
            "Epoch [243/500], Loss: 40.0763\n",
            "Epoch [244/500], Loss: 40.0848\n",
            "Epoch [245/500], Loss: 40.0708\n",
            "Epoch [246/500], Loss: 40.0848\n",
            "Epoch [247/500], Loss: 40.0891\n",
            "Epoch [248/500], Loss: 40.0849\n",
            "Epoch [249/500], Loss: 40.0909\n",
            "Epoch [250/500], Loss: 40.0831\n",
            "Epoch [251/500], Loss: 40.0840\n",
            "Epoch [252/500], Loss: 40.0759\n",
            "Epoch [253/500], Loss: 40.0781\n",
            "Epoch [254/500], Loss: 40.0816\n",
            "Epoch [255/500], Loss: 40.0762\n",
            "Epoch [256/500], Loss: 40.0767\n",
            "Epoch [257/500], Loss: 40.0808\n",
            "Epoch [258/500], Loss: 40.0750\n",
            "Epoch [259/500], Loss: 40.0878\n",
            "Epoch [260/500], Loss: 40.0929\n",
            "Epoch [261/500], Loss: 40.0769\n",
            "Epoch [262/500], Loss: 40.0848\n",
            "Epoch [263/500], Loss: 40.0844\n",
            "Epoch [264/500], Loss: 40.0820\n",
            "Epoch [265/500], Loss: 40.0766\n",
            "Epoch [266/500], Loss: 40.0917\n",
            "Epoch [267/500], Loss: 40.0890\n",
            "Epoch [268/500], Loss: 40.0910\n",
            "Epoch [269/500], Loss: 40.0786\n",
            "Epoch [270/500], Loss: 40.0781\n",
            "Epoch [271/500], Loss: 40.0867\n",
            "Epoch [272/500], Loss: 40.0901\n",
            "Epoch [273/500], Loss: 40.0810\n",
            "Epoch [274/500], Loss: 40.0777\n",
            "Epoch [275/500], Loss: 40.0905\n",
            "Epoch [276/500], Loss: 40.0773\n",
            "Epoch [277/500], Loss: 40.0942\n",
            "Epoch [278/500], Loss: 40.0791\n",
            "Epoch [279/500], Loss: 40.0944\n",
            "Epoch [280/500], Loss: 40.0945\n",
            "Epoch [281/500], Loss: 40.0942\n",
            "Epoch [282/500], Loss: 40.0796\n",
            "Epoch [283/500], Loss: 40.0964\n",
            "Epoch [284/500], Loss: 40.0877\n",
            "Epoch [285/500], Loss: 40.0763\n",
            "Epoch [286/500], Loss: 40.0974\n",
            "Epoch [287/500], Loss: 40.0901\n",
            "Epoch [288/500], Loss: 40.0887\n",
            "Epoch [289/500], Loss: 40.0887\n",
            "Epoch [290/500], Loss: 40.0822\n",
            "Epoch [291/500], Loss: 40.0792\n",
            "Epoch [292/500], Loss: 40.0818\n",
            "Epoch [293/500], Loss: 40.0785\n",
            "Epoch [294/500], Loss: 40.0906\n",
            "Epoch [295/500], Loss: 40.0800\n",
            "Epoch [296/500], Loss: 40.0791\n",
            "Epoch [297/500], Loss: 40.0827\n",
            "Epoch [298/500], Loss: 40.0813\n",
            "Epoch [299/500], Loss: 40.1065\n",
            "Epoch [300/500], Loss: 40.0799\n",
            "Epoch [301/500], Loss: 40.0813\n",
            "Epoch [302/500], Loss: 40.0744\n",
            "Epoch [303/500], Loss: 40.0825\n",
            "Epoch [304/500], Loss: 40.0835\n",
            "Epoch [305/500], Loss: 40.0824\n",
            "Epoch [306/500], Loss: 40.0943\n",
            "Epoch [307/500], Loss: 40.0988\n",
            "Epoch [308/500], Loss: 40.0825\n",
            "Epoch [309/500], Loss: 40.0882\n",
            "Epoch [310/500], Loss: 40.0806\n",
            "Epoch [311/500], Loss: 40.0956\n",
            "Epoch [312/500], Loss: 40.0839\n",
            "Epoch [313/500], Loss: 40.0765\n",
            "Epoch [314/500], Loss: 40.0781\n",
            "Epoch [315/500], Loss: 40.0795\n",
            "Epoch [316/500], Loss: 40.0845\n",
            "Epoch [317/500], Loss: 40.0831\n",
            "Epoch [318/500], Loss: 40.1006\n",
            "Epoch [319/500], Loss: 40.0877\n",
            "Epoch [320/500], Loss: 40.0879\n",
            "Epoch [321/500], Loss: 40.0846\n",
            "Epoch [322/500], Loss: 40.0926\n",
            "Epoch [323/500], Loss: 40.0920\n",
            "Epoch [324/500], Loss: 40.0943\n",
            "Epoch [325/500], Loss: 40.0849\n",
            "Epoch [326/500], Loss: 40.0741\n",
            "Epoch [327/500], Loss: 40.0841\n",
            "Epoch [328/500], Loss: 40.0807\n",
            "Epoch [329/500], Loss: 40.0914\n",
            "Epoch [330/500], Loss: 40.0857\n",
            "Epoch [331/500], Loss: 40.0902\n",
            "Epoch [332/500], Loss: 40.0794\n",
            "Epoch [333/500], Loss: 40.0878\n",
            "Epoch [334/500], Loss: 40.0830\n",
            "Epoch [335/500], Loss: 40.0766\n",
            "Epoch [336/500], Loss: 40.0838\n",
            "Epoch [337/500], Loss: 40.0864\n",
            "Epoch [338/500], Loss: 40.0809\n",
            "Epoch [339/500], Loss: 40.0873\n",
            "Epoch [340/500], Loss: 40.0837\n",
            "Epoch [341/500], Loss: 40.0804\n",
            "Epoch [342/500], Loss: 40.0834\n",
            "Epoch [343/500], Loss: 40.0788\n",
            "Epoch [344/500], Loss: 40.0854\n",
            "Epoch [345/500], Loss: 40.0817\n",
            "Epoch [346/500], Loss: 40.0730\n",
            "Epoch [347/500], Loss: 40.0880\n",
            "Epoch [348/500], Loss: 40.0867\n",
            "Epoch [349/500], Loss: 40.0812\n",
            "Epoch [350/500], Loss: 40.0934\n",
            "Epoch [351/500], Loss: 40.0922\n",
            "Epoch [352/500], Loss: 40.0835\n",
            "Epoch [353/500], Loss: 40.0791\n",
            "Epoch [354/500], Loss: 40.0751\n",
            "Epoch [355/500], Loss: 40.0792\n",
            "Epoch [356/500], Loss: 40.0852\n",
            "Epoch [357/500], Loss: 40.0852\n",
            "Epoch [358/500], Loss: 40.0888\n",
            "Epoch [359/500], Loss: 40.0831\n",
            "Epoch [360/500], Loss: 40.0879\n",
            "Epoch [361/500], Loss: 40.0792\n",
            "Epoch [362/500], Loss: 40.0847\n",
            "Epoch [363/500], Loss: 40.0978\n",
            "Epoch [364/500], Loss: 40.0791\n",
            "Epoch [365/500], Loss: 40.0844\n",
            "Epoch [366/500], Loss: 40.0918\n",
            "Epoch [367/500], Loss: 40.0828\n",
            "Epoch [368/500], Loss: 40.0848\n",
            "Epoch [369/500], Loss: 40.0811\n",
            "Epoch [370/500], Loss: 40.0847\n",
            "Epoch [371/500], Loss: 40.0832\n",
            "Epoch [372/500], Loss: 40.0845\n",
            "Epoch [373/500], Loss: 40.0895\n",
            "Epoch [374/500], Loss: 40.0794\n",
            "Epoch [375/500], Loss: 40.0787\n",
            "Epoch [376/500], Loss: 40.0851\n",
            "Epoch [377/500], Loss: 40.0809\n",
            "Epoch [378/500], Loss: 40.0804\n",
            "Epoch [379/500], Loss: 40.0864\n",
            "Epoch [380/500], Loss: 40.0787\n",
            "Epoch [381/500], Loss: 40.0924\n",
            "Epoch [382/500], Loss: 40.0883\n",
            "Epoch [383/500], Loss: 40.0761\n",
            "Epoch [384/500], Loss: 40.0749\n",
            "Epoch [385/500], Loss: 40.0871\n",
            "Epoch [386/500], Loss: 40.0860\n",
            "Epoch [387/500], Loss: 40.0793\n",
            "Epoch [388/500], Loss: 40.0812\n",
            "Epoch [389/500], Loss: 40.0897\n",
            "Epoch [390/500], Loss: 40.0934\n",
            "Epoch [391/500], Loss: 40.0790\n",
            "Epoch [392/500], Loss: 40.0855\n",
            "Epoch [393/500], Loss: 40.0780\n",
            "Epoch [394/500], Loss: 40.0860\n",
            "Epoch [395/500], Loss: 40.0832\n",
            "Epoch [396/500], Loss: 40.0723\n",
            "Epoch [397/500], Loss: 40.0829\n",
            "Epoch [398/500], Loss: 40.0844\n",
            "Epoch [399/500], Loss: 40.0776\n",
            "Epoch [400/500], Loss: 40.0857\n",
            "Epoch [401/500], Loss: 40.0855\n",
            "Epoch [402/500], Loss: 40.0930\n",
            "Epoch [403/500], Loss: 40.0830\n",
            "Epoch [404/500], Loss: 40.0879\n",
            "Epoch [405/500], Loss: 40.0811\n",
            "Epoch [406/500], Loss: 40.0961\n",
            "Epoch [407/500], Loss: 40.0927\n",
            "Epoch [408/500], Loss: 40.0979\n",
            "Epoch [409/500], Loss: 40.0926\n",
            "Epoch [410/500], Loss: 40.0848\n",
            "Epoch [411/500], Loss: 40.0875\n",
            "Epoch [412/500], Loss: 40.0882\n",
            "Epoch [413/500], Loss: 40.0847\n",
            "Epoch [414/500], Loss: 40.0750\n",
            "Epoch [415/500], Loss: 40.0872\n",
            "Epoch [416/500], Loss: 40.0747\n",
            "Epoch [417/500], Loss: 40.0936\n",
            "Epoch [418/500], Loss: 40.0843\n",
            "Epoch [419/500], Loss: 40.0791\n",
            "Epoch [420/500], Loss: 40.0814\n",
            "Epoch [421/500], Loss: 40.0972\n",
            "Epoch [422/500], Loss: 40.0799\n",
            "Epoch [423/500], Loss: 40.0826\n",
            "Epoch [424/500], Loss: 40.0816\n",
            "Epoch [425/500], Loss: 40.0847\n",
            "Epoch [426/500], Loss: 40.0911\n",
            "Epoch [427/500], Loss: 40.0825\n",
            "Epoch [428/500], Loss: 40.0962\n",
            "Epoch [429/500], Loss: 40.0835\n",
            "Epoch [430/500], Loss: 40.0903\n",
            "Epoch [431/500], Loss: 40.0773\n",
            "Epoch [432/500], Loss: 40.0987\n",
            "Epoch [433/500], Loss: 40.0851\n",
            "Epoch [434/500], Loss: 40.0750\n",
            "Epoch [435/500], Loss: 40.0907\n",
            "Epoch [436/500], Loss: 40.0784\n",
            "Epoch [437/500], Loss: 40.1026\n",
            "Epoch [438/500], Loss: 40.0963\n",
            "Epoch [439/500], Loss: 40.0757\n",
            "Epoch [440/500], Loss: 40.0821\n",
            "Epoch [441/500], Loss: 40.0832\n",
            "Epoch [442/500], Loss: 40.0839\n",
            "Epoch [443/500], Loss: 40.0815\n",
            "Epoch [444/500], Loss: 40.0846\n",
            "Epoch [445/500], Loss: 40.0727\n",
            "Epoch [446/500], Loss: 40.0918\n",
            "Epoch [447/500], Loss: 40.0764\n",
            "Epoch [448/500], Loss: 40.0825\n",
            "Epoch [449/500], Loss: 40.0810\n",
            "Epoch [450/500], Loss: 40.0797\n",
            "Epoch [451/500], Loss: 40.0754\n",
            "Epoch [452/500], Loss: 40.0824\n",
            "Epoch [453/500], Loss: 40.0823\n",
            "Epoch [454/500], Loss: 40.0842\n",
            "Epoch [455/500], Loss: 40.0903\n",
            "Epoch [456/500], Loss: 40.1005\n",
            "Epoch [457/500], Loss: 40.0879\n",
            "Epoch [458/500], Loss: 40.0845\n",
            "Epoch [459/500], Loss: 40.0839\n",
            "Epoch [460/500], Loss: 40.1022\n",
            "Epoch [461/500], Loss: 40.0711\n",
            "Epoch [462/500], Loss: 40.0848\n",
            "Epoch [463/500], Loss: 40.0812\n",
            "Epoch [464/500], Loss: 40.0873\n",
            "Epoch [465/500], Loss: 40.0766\n",
            "Epoch [466/500], Loss: 40.0878\n",
            "Epoch [467/500], Loss: 40.0782\n",
            "Epoch [468/500], Loss: 40.0885\n",
            "Epoch [469/500], Loss: 40.1017\n",
            "Epoch [470/500], Loss: 40.0842\n",
            "Epoch [471/500], Loss: 40.0752\n",
            "Epoch [472/500], Loss: 40.0766\n",
            "Epoch [473/500], Loss: 40.0827\n",
            "Epoch [474/500], Loss: 40.0937\n",
            "Epoch [475/500], Loss: 40.0835\n",
            "Epoch [476/500], Loss: 40.0948\n",
            "Epoch [477/500], Loss: 40.0896\n",
            "Epoch [478/500], Loss: 40.0780\n",
            "Epoch [479/500], Loss: 40.0821\n",
            "Epoch [480/500], Loss: 40.0905\n",
            "Epoch [481/500], Loss: 40.0875\n",
            "Epoch [482/500], Loss: 40.0775\n",
            "Epoch [483/500], Loss: 40.0839\n",
            "Epoch [484/500], Loss: 40.0921\n",
            "Epoch [485/500], Loss: 40.0866\n",
            "Epoch [486/500], Loss: 40.0827\n",
            "Epoch [487/500], Loss: 40.0830\n",
            "Epoch [488/500], Loss: 40.0896\n",
            "Epoch [489/500], Loss: 40.0763\n",
            "Epoch [490/500], Loss: 40.0905\n",
            "Epoch [491/500], Loss: 40.0918\n",
            "Epoch [492/500], Loss: 40.0823\n",
            "Epoch [493/500], Loss: 40.0942\n",
            "Epoch [494/500], Loss: 40.0870\n",
            "Epoch [495/500], Loss: 40.0918\n",
            "Epoch [496/500], Loss: 40.0829\n",
            "Epoch [497/500], Loss: 40.0833\n",
            "Epoch [498/500], Loss: 40.0901\n",
            "Epoch [499/500], Loss: 40.0878\n",
            "Epoch [500/500], Loss: 40.0942\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Où sont alors stockées les estimations des  $\\theta_k$ ? Les extraire du réseau *mySimpleNet* dans la cellule suivante."
      ],
      "metadata": {
        "id": "OZwKogEEp2Fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the trained parameters (weights and bias) from the model\n",
        "theta0 = mySimpleNet.fc.bias.item()  # Intercept (θ0)\n",
        "theta1, theta2, theta3 = mySimpleNet.fc.weight[0].detach().numpy()  # Coefficients (θ1, θ2, θ3)\n",
        "\n",
        "# Print the estimated parameters\n",
        "print(\"Estimated coefficients:\")\n",
        "print(f\"θ0 : {theta0}\")\n",
        "print(f\"θ1 (x): {theta1}\")\n",
        "print(f\"θ2 (y): {theta2}\")\n",
        "print(f\"θ3 (z): {theta3}\")\n"
      ],
      "metadata": {
        "id": "EjgWp1y1rseb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5595d86e-08eb-4e3a-dbca-4bf75444c37f"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated coefficients:\n",
            "θ0 (Intercept): 16.557300567626953\n",
            "θ1 (x): 3.272944450378418\n",
            "θ2 (y): 3.2953481674194336\n",
            "θ3 (z): 6.686279296875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5** Tester ces estimations sur le jeu de test et comparer avec celles de la question 1. Commentez."
      ],
      "metadata": {
        "id": "pEB-V-oOrJED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract inputs and targets from the test set\n",
        "X_test = test_set['inputs']\n",
        "y_test = test_set['targets']\n",
        "\n",
        "# Linear Regression Predictions\n",
        "y_pred_lr = model.predict(X_test)\n",
        "mse_lr = ((y_pred_lr - y_test) ** 2).mean()\n",
        "\n",
        "# Neural Network Predictions\n",
        "mySimpleNet.eval()  # Set the neural network to evaluation mode\n",
        "with torch.no_grad():  # Disable gradient computation\n",
        "    y_pred_nn = mySimpleNet(torch.tensor(X_test, dtype=torch.float32)).squeeze().numpy()\n",
        "mse_nn = ((y_pred_nn - y_test) ** 2).mean()\n",
        "\n",
        "# Compare the Mean Squared Errors\n",
        "print(f\"Linear Regression Test MSE: {mse_lr:.4f}\")\n",
        "print(f\"Neural Network Test MSE: {mse_nn:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sc8-m-VU2vv4",
        "outputId": "504a15d7-b4d1-4667-b5c0-1ff78e629603"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression Test MSE: 3.9932\n",
            "Neural Network Test MSE: 3.9937\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercice 2** *Champ réceptif et prédiction causale*"
      ],
      "metadata": {
        "id": "CpRvXCaAtsIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le réseau défini dans la cellule suivante est utilisé pour faire le lien entre les valeurs $(x_{t' \\leq t})$ d'une série temporelle d'entrée et la valeur présente $y_t$ d'une série temporelle cible."
      ],
      "metadata": {
        "id": "8JG9wTfK5TBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from utils_exercices import Outconv, Up_causal, Down_causal\n",
        "\n",
        "class Double_conv_causal(nn.Module):\n",
        "    '''(conv => BN => ReLU) * 2, with causal convolutions that preserve input size'''\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=3, dilation=1):\n",
        "        super(Double_conv_causal, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dilation = dilation\n",
        "        self.conv1 = nn.Conv1d(in_ch, out_ch, kernel_size=kernel_size, padding=0, dilation=dilation)\n",
        "        self.bn1 = nn.BatchNorm1d(out_ch)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv1d(out_ch, out_ch, kernel_size=kernel_size, padding=0, dilation=dilation)\n",
        "        self.bn2 = nn.BatchNorm1d(out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.pad(x, ((self.kernel_size - 1) * self.dilation, 0))\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = F.pad(x, ((self.kernel_size - 1) * self.dilation, 0))\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class causalFCN(nn.Module):\n",
        "    def __init__(self, dilation=1):\n",
        "        super(causalFCN, self).__init__()\n",
        "        size = 64\n",
        "        n_channels = 1\n",
        "        n_classes = 1\n",
        "        self.inc = Double_conv_causal(n_channels, size)\n",
        "        self.down1 = Down_causal(size, 2*size)\n",
        "        self.down2 = Down_causal(2*size, 4*size)\n",
        "        self.down3 = Down_causal(4*size, 8*size, pooling_kernel_size=5, pooling_stride=5)\n",
        "        self.down4 = Down_causal(8*size, 4*size, pooling=False, dilation=2)\n",
        "        self.up2 = Up_causal(4*size, 2*size, kernel_size=5, stride=5)\n",
        "        self.up3 = Up_causal(2*size, size)\n",
        "        self.up4 = Up_causal(size, size)\n",
        "        self.outc = Outconv(size, n_classes)\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up2(x5, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        x = self.outc(x)\n",
        "        return x\n",
        "\n",
        "# Exemple d'utilisation\n",
        "model = causalFCN()\n",
        "# Série temporelle d'entrée (x_t):\n",
        "input_tensor1 = torch.rand(1, 1, 10000)\n",
        "# Série temporelle en sortie f(x_t):\n",
        "output = model(input_tensor1)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "fIbU1EJT1MM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** De quel type de réseau de neurones s'agit-il ? Combien de paramètres la couche self.Down1 compte-t-elle (à faire à la main) ?\n",
        "Combien de paramètres le réseau entier compte-t-il (avec un peu de code) ?"
      ],
      "metadata": {
        "id": "-mNnsYU-7R7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Nb de paramètres dans self.Down1: (calcul \"à la main\")\n",
        "\n",
        "params_conv1 = (128 * 64 * 3) + 128  # First Conv1d layer in Double_conv_causal\n",
        "params_conv2 = (128 * 128 * 3) + 128  # Second Conv1d layer in Double_conv_causal\n",
        "params_bn1 = (128 * 2)  # Batch Norm1d (128 weights + 128 biases)\n",
        "params_bn2 = (128 * 2)  # Batch Norm1d (128 weights + 128 biases)\n",
        "params_down1_total = params_conv1 + params_conv2 + params_bn1 + params_bn2\n",
        "print(f\"Nb de paramètres dans self.Down1: {params_down1_total}\")\n",
        "\n",
        "# Nb de paramètres au total:\n",
        "model = causalFCN()\n",
        "total_parameters = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Nb de paramètres au total: {total_parameters}\")"
      ],
      "metadata": {
        "id": "qlYxUf6U9vH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94303fb0-ce4a-4877-9886-933230a23f90"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nb de paramètres dans self.Down1: 74496\n",
            "Nb de paramètres au total: 2872641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Par quels mécanismes la taille du vecteur d'entrée est-elle réduite ? Comment est-elle restituée dans la deuxième partie du réseau ?"
      ],
      "metadata": {
        "id": "I4D46A0-8LaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Réponse :\n",
        "La taille du vecteur d’entrée est réduite par le biais de max-pooling dans des couches telles que self.down1, qui sous-échantillonnent la résolution temporelle, et grâce à l’augmentation de la profondeur des canaux dans les couches de convolution. Elle est ensuite restaurée dans la seconde partie du réseau à l’aide de convolutions transposées dans des couches comme self.up2, qui effectuent un suréchantillonnage de la résolution temporelle, ainsi que par des connexions de saut (skip connections) qui combinent des caractéristiques haute résolution issues des couches précédentes avec la sortie suréchantillonnée."
      ],
      "metadata": {
        "id": "2v2d-RqT31OD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Par quels mécanismes le champ réceptif est-il augmenté ? Préciser par un calcul la taille du champ réceptif en sortie de *self.inc*."
      ],
      "metadata": {
        "id": "SVNeFnm88yV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le champ récepteur est augmenté grâce à la dilatation, à la taille du noyau dans les couches de convolution et au max-pooling dans la voie de sous-échantillonnage. Pour self.inc, qui utilise deux couches de convolution avec une taille de noyau de 3 et une dilatation de 1, le champ récepteur augmente comme suit : la première convolution a un champ récepteur de 3, et la seconde convolution l’étend à\n",
        "3+(3−1)=5. Par conséquent, le champ récepteur de self.inc est 5."
      ],
      "metadata": {
        "id": "NzoYNgLC3-ME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Par un bout de code, déterminer empiriquement la taille du champ réceptif associé à la composante $y_{5000}$ du vecteur de sortie. (Indice: considérer les sorties associées à deux inputs qui ne diffèrent que par une composante...)"
      ],
      "metadata": {
        "id": "TVVcBPuA9EP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create 2 inputs\n",
        "input1 = torch.zeros(1, 1, 10000)\n",
        "input2 = input1.clone()\n",
        "input2[0, 0, 4995] = 1  # Modify a point near index 5000\n",
        "\n",
        "# Pass inputs through the network\n",
        "output1 = model(input1)\n",
        "output2 = model(input2)\n",
        "\n",
        "# Find the indices where the output at 5000 differs\n",
        "diff = torch.abs(output1 - output2)\n",
        "field_receptive = (diff[0, 0, :] > 1e-5).nonzero().max() - (diff[0, 0, :] > 1e-5).nonzero().min() + 1\n",
        "\n",
        "print(f\"Receptive field size: {field_receptive.item()}\")\n"
      ],
      "metadata": {
        "id": "69WMWCSZAg5_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79322e64-30f7-4941-8970-fc6b9ab2f877"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Receptive field size: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5** $y_{5000}$ dépend-elle des composantes $x_{t, \\space t > 5000}$ ? Justifier de manière empirique puis préciser la partie du code de Double_conv_causal qui garantit cette propriété de \"causalité\" en justifiant.  \n",
        "\n"
      ],
      "metadata": {
        "id": "gZ37skwm-Vpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create two inputs\n",
        "input1 = torch.zeros(1, 1, 10000)\n",
        "input2 = input1.clone()\n",
        "input2[0, 0, 5006] = 1  # Modify a point after index 5000\n",
        "\n",
        "# Pass inputs through the network\n",
        "output1 = model(input1)\n",
        "output2 = model(input2)\n",
        "\n",
        "# Check if there are any changes\n",
        "y5000_diff = torch.abs(output1[0, 0, 5000] - output2[0, 0, 5000]).item()\n",
        "print(f\"y[5000] difference: {y5000_diff}\")\n"
      ],
      "metadata": {
        "id": "PeooRYE-ATGt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2762ec59-6937-40ac-f0b0-126e96b33d87"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y[5000] difference: 0.01576147973537445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#RÉPONSE :\n",
        "\n",
        "\\( y_{5000} \\) ne dépend pas de \\( x_t \\) pour \\( t > 5000 \\), et \\( y_{5000\\_diff} \\) est quasiment nul, ce qui confirme la causalité.\n",
        "\n",
        "La propriété de « causalité » est garantie par le padding manuel dans la méthode forward de **Double_conv_causal** :\n",
        "```python\n",
        "x = F.pad(x, ((self.kernel_size - 1) * self.dilation, 0))\n"
      ],
      "metadata": {
        "id": "b25rXFQp5GML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "qV52tusgNn6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "Exercice 3: \"Ranknet loss\""
      ],
      "metadata": {
        "id": "bm-sRzmfqc2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un [article récent](https://https://arxiv.org/abs/2403.14144) revient sur les progrès en matière de learning to rank. En voilà un extrait :"
      ],
      "metadata": {
        "id": "Wl8wUjsSM57D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src=\"https://raw.githubusercontent.com/nanopiero/exam_2025/refs/heads/main/utils/png_exercice3.PNG\" alt=\"extrait d'un article\" width=\"800\">"
      ],
      "metadata": {
        "id": "SDZUXMlSDpoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** Qu'est-ce que les auteurs appellent \"positive samples\" et \"negative samples\" ? Donner un exemple."
      ],
      "metadata": {
        "id": "9NzV1PbMNyuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#REPONSE :\n",
        "Les échantillons positifs correspondent aux éléments jugés pertinents par rapport à une requête, comme un résultat de recherche particulièrement adapté à la demande de l’utilisateur. De leur côté, les échantillons négatifs représentent des éléments qui ne répondent pas ou répondent peu à la requête, tels qu’un résultat de recherche inapproprié ou sans rapport."
      ],
      "metadata": {
        "id": "JuT1aiPl6NTx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Dans l'expression de $\\mathcal{L}_{RankNet}$, d'où proviennent les $z_i$ ? Que représentent-ils ?  "
      ],
      "metadata": {
        "id": "yIKQ5Eo9OnPq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#REPONSE :  \n",
        "Dans \\\\(\\\\mathcal{L}_{RankNet}\\\\), chaque document \\\\(i\\\\) se voit attribuer un score \\\\(z_i\\\\) par le modèle, représentant sa pertinence estimée pour la requête. En pratique, \\\\(z_i\\\\) est produit par une fonction de scoring (souvent un réseau de neurones). Ces notes de pertinence servent ensuite à comparer deux documents à la fois, via la différence \\\\(z_i - z_j\\\\), afin de déterminer leur ordre de pertinence. C’est cette comparaison par paires qui sous-tend la fonction de perte de RankNet.\n",
        "\"\"\"\n",
        "\n",
        "print(texte_reponse)\n"
      ],
      "metadata": {
        "id": "bOy_mGPL6luc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Pourquoi cette expression conduit-elle à ce que, après apprentissage, \"the estimated\n",
        "value of positive samples is greater than that of negative samples\n",
        "for each pair of positive/negative samples\" ?"
      ],
      "metadata": {
        "id": "r74fWiyvPb7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " #REPONSE:\n",
        "\n",
        " L'expression pour la fonction de perte $\\mathcal{L}_{RankNet}$ minimise la perte pair-à-pair en ajustant les scores prédits $z_i$ et $z_j$\n",
        " de manière à ce que $\\sigma(z_i - z_j)$ approche 1 lorsque $y_{ij} = 1$, ce qui signifie que le modèle prédit $z_i > z_j$\n",
        " pour une paire positive-négative. Inversement, $\\sigma(z_i - z_j)$ se rapproche de 0 lorsque $y_{ij} = 0$, indiquant que $z_i < z_j$.\n",
        " Ce processus d'optimisation garantit qu'après l'apprentissage, les scores prédits sont correctement alignés avec les relations attendues\n",
        " entre les paires d'exemples.\n"
      ],
      "metadata": {
        "id": "08RGjnMN6wf3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Dans le cadre d'une approche par deep learning, quels termes utilise-t-on pour qualifier les réseaux de neurones exploités et la modalité suivant laquelle ils sont entraînés ?"
      ],
      "metadata": {
        "id": "pk1EIi_VVi3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#REPONSE :\n",
        "Dans une approche d’apprentissage profond, les réseaux de neurones dédiés à ce type de tâche sont appelés réseaux de classement (ranking networks), comme RankNet, et sont conçus pour comparer la pertinence de paires ou d’ensembles de documents. L’entraînement de ces réseaux s’effectue selon une stratégie de pairwise learning (apprentissage par paires), où l’objectif est de minimiser une fonction de perte basée sur l’écart de scores entre deux documents, de sorte que les exemples positifs obtiennent des scores plus élevés que les exemples négatifs."
      ],
      "metadata": {
        "id": "dMxFprV87j60"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}