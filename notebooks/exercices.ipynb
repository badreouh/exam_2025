{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CACBFsndOCo"
      },
      "source": [
        "# Exercices"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Préliminaires**: Clone de votre repo et imports"
      ],
      "metadata": {
        "id": "hfkMtaHleKAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/XXXXXXX/exam_2025.git\n",
        "! cp exam_2025/utils/utils_exercices.py .\n",
        "\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "xiD_cI-geJjI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4a0ca8a-5aaa-4604-f0a5-af6da5bbefbd"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'exam_2025' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clef personnelle pour la partie théorique**\n",
        "\n",
        "Dans la cellule suivante, choisir un entier entre 100 et 1000 (il doit être personnel). Cet entier servira de graine au générateur de nombres aléatoire a conserver pour tous les exercices.\n",
        "\n"
      ],
      "metadata": {
        "id": "J3ga_6BNc5DR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mySeed = 333"
      ],
      "metadata": {
        "id": "PrCTHM4od5UZ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "TRWBLVpCWC06"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5RcggmAkJLV"
      },
      "source": [
        "\\\n",
        "\n",
        "**Exercice 1** *Une relation linéaire*\n",
        "\n",
        "La fonction *generate_dataset* fournit deux jeux de données (entraînement et test). Pour chaque jeu de données, la clef 'inputs' donne accès à un tableau numpy (numpy array) de prédicteurs empilés horizontalement : chaque ligne $i$ contient trois prédicteurs $x_i$, $y_i$ et $z_i$. La clef 'targets' renvoie le vecteur des cibles $t_i$. \\\n",
        "\n",
        "Les cibles sont liées aux prédicteurs par le modèle:\n",
        "$$ t = \\theta_0 + \\theta_1 x + \\theta_2 y + \\theta_3 z + \\epsilon$$ où $\\epsilon \\sim \\mathcal{N}(0,\\eta)$\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils_exercices import generate_dataset, Dataset1\n",
        "train_set, test_set = generate_dataset(mySeed)"
      ],
      "metadata": {
        "id": "gEQmgTI8my8i"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** Par quelle méthode simple peut-on estimer les coefficients $\\theta_k$ ? La mettre en oeuvre avec la librairie python de votre choix."
      ],
      "metadata": {
        "id": "q5XZTrXNk12K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "# Extract inputs and targets from the training set\n",
        "X_train = train_set['inputs']\n",
        "y_train = train_set['targets']\n",
        "\n",
        "# Train a linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Retrieve the coefficients\n",
        "theta0 = model.intercept_  # This is θ0\n",
        "theta1, theta2, theta3 = model.coef_  # These are θ1, θ2, θ3\n",
        "\n",
        "# Display the results\n",
        "print(f\"Estimated coefficients:\")\n",
        "print(f\"θ0 (Intercept): {theta0}\")\n",
        "print(f\"θ1 (x): {theta1}\")\n",
        "print(f\"θ2 (y): {theta2}\")\n",
        "print(f\"θ3 (z): {theta3}\")\n"
      ],
      "metadata": {
        "id": "HITtUqHhFMkn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5753670-4c1c-4b4b-b15d-98ce179ca466"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated coefficients:\n",
            "θ0 (Intercept): 16.558024510983287\n",
            "θ1 (x): 3.2729838244521834\n",
            "θ2 (y): 3.294835451763797\n",
            "θ3 (z): 6.687036331785982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MXGXg8tlPULY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Dans les cellules suivantes, on se propose d'estimer les $\\theta_k$ grâce à un réseau de neurones entraîné par SGD. Quelle architecture s'y prête ? Justifier en termes d'expressivité et de performances en généralisation puis la coder dans la cellule suivante."
      ],
      "metadata": {
        "id": "CH_Z5ZEIlQPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset et dataloader :\n",
        "dataset = Dataset1(train_set['inputs'], train_set['targets'])\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=100, shuffle=True)\n",
        "\n",
        "# A coder :\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.fc = nn.Linear(3, 1)  # Linear layer for 3 inputs (x, y, z) and 1 output (t)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)  # Return the linear combination"
      ],
      "metadata": {
        "id": "PPx543blnxdb"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Entraîner cette architecture à la tâche de régression définie par les entrées et sorties du jeu d'entraînement (compléter la cellule ci-dessous)."
      ],
      "metadata": {
        "id": "g6BSTBitpGBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model, loss, and optimizer\n",
        "mySimpleNet = SimpleNet()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(mySimpleNet.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 500\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0.0  # Track total loss for this epoch\n",
        "    for batch_inputs, batch_targets in dataloader:\n",
        "        optimizer.zero_grad()  # Reset gradients to zero\n",
        "\n",
        "        # Forward pass: compute predictions\n",
        "        outputs = mySimpleNet(batch_inputs)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(outputs.squeeze(), batch_targets)\n",
        "\n",
        "        # Backward pass: compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate loss for reporting\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    # Print the epoch loss\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}\")"
      ],
      "metadata": {
        "id": "Wjfa2Z4RoPO-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45b2c646-fcc3-4f09-a787-9fd6a53f0f81"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/500], Loss: 3666.6341\n",
            "Epoch [2/500], Loss: 2182.7175\n",
            "Epoch [3/500], Loss: 1310.4595\n",
            "Epoch [4/500], Loss: 796.9395\n",
            "Epoch [5/500], Loss: 494.2735\n",
            "Epoch [6/500], Loss: 315.2254\n",
            "Epoch [7/500], Loss: 209.0695\n",
            "Epoch [8/500], Loss: 145.6608\n",
            "Epoch [9/500], Loss: 107.6456\n",
            "Epoch [10/500], Loss: 84.6024\n",
            "Epoch [11/500], Loss: 70.5605\n",
            "Epoch [12/500], Loss: 61.8428\n",
            "Epoch [13/500], Loss: 56.3595\n",
            "Epoch [14/500], Loss: 52.8258\n",
            "Epoch [15/500], Loss: 50.4447\n",
            "Epoch [16/500], Loss: 48.7931\n",
            "Epoch [17/500], Loss: 47.6048\n",
            "Epoch [18/500], Loss: 46.6966\n",
            "Epoch [19/500], Loss: 45.9841\n",
            "Epoch [20/500], Loss: 45.4029\n",
            "Epoch [21/500], Loss: 44.9051\n",
            "Epoch [22/500], Loss: 44.4917\n",
            "Epoch [23/500], Loss: 44.1082\n",
            "Epoch [24/500], Loss: 43.7661\n",
            "Epoch [25/500], Loss: 43.4658\n",
            "Epoch [26/500], Loss: 43.1821\n",
            "Epoch [27/500], Loss: 42.9485\n",
            "Epoch [28/500], Loss: 42.7118\n",
            "Epoch [29/500], Loss: 42.5047\n",
            "Epoch [30/500], Loss: 42.3113\n",
            "Epoch [31/500], Loss: 42.1442\n",
            "Epoch [32/500], Loss: 41.9698\n",
            "Epoch [33/500], Loss: 41.8305\n",
            "Epoch [34/500], Loss: 41.6865\n",
            "Epoch [35/500], Loss: 41.5588\n",
            "Epoch [36/500], Loss: 41.4506\n",
            "Epoch [37/500], Loss: 41.3449\n",
            "Epoch [38/500], Loss: 41.2365\n",
            "Epoch [39/500], Loss: 41.1425\n",
            "Epoch [40/500], Loss: 41.0580\n",
            "Epoch [41/500], Loss: 40.9825\n",
            "Epoch [42/500], Loss: 40.9102\n",
            "Epoch [43/500], Loss: 40.8482\n",
            "Epoch [44/500], Loss: 40.7829\n",
            "Epoch [45/500], Loss: 40.7280\n",
            "Epoch [46/500], Loss: 40.6857\n",
            "Epoch [47/500], Loss: 40.6354\n",
            "Epoch [48/500], Loss: 40.5867\n",
            "Epoch [49/500], Loss: 40.5617\n",
            "Epoch [50/500], Loss: 40.5175\n",
            "Epoch [51/500], Loss: 40.4824\n",
            "Epoch [52/500], Loss: 40.4460\n",
            "Epoch [53/500], Loss: 40.4191\n",
            "Epoch [54/500], Loss: 40.4022\n",
            "Epoch [55/500], Loss: 40.3662\n",
            "Epoch [56/500], Loss: 40.3392\n",
            "Epoch [57/500], Loss: 40.3390\n",
            "Epoch [58/500], Loss: 40.3124\n",
            "Epoch [59/500], Loss: 40.2825\n",
            "Epoch [60/500], Loss: 40.2796\n",
            "Epoch [61/500], Loss: 40.2590\n",
            "Epoch [62/500], Loss: 40.2467\n",
            "Epoch [63/500], Loss: 40.2433\n",
            "Epoch [64/500], Loss: 40.2120\n",
            "Epoch [65/500], Loss: 40.2160\n",
            "Epoch [66/500], Loss: 40.1966\n",
            "Epoch [67/500], Loss: 40.1888\n",
            "Epoch [68/500], Loss: 40.1810\n",
            "Epoch [69/500], Loss: 40.1712\n",
            "Epoch [70/500], Loss: 40.1668\n",
            "Epoch [71/500], Loss: 40.1595\n",
            "Epoch [72/500], Loss: 40.1525\n",
            "Epoch [73/500], Loss: 40.1514\n",
            "Epoch [74/500], Loss: 40.1445\n",
            "Epoch [75/500], Loss: 40.1333\n",
            "Epoch [76/500], Loss: 40.1310\n",
            "Epoch [77/500], Loss: 40.1302\n",
            "Epoch [78/500], Loss: 40.1206\n",
            "Epoch [79/500], Loss: 40.1224\n",
            "Epoch [80/500], Loss: 40.1102\n",
            "Epoch [81/500], Loss: 40.1236\n",
            "Epoch [82/500], Loss: 40.1110\n",
            "Epoch [83/500], Loss: 40.1191\n",
            "Epoch [84/500], Loss: 40.1146\n",
            "Epoch [85/500], Loss: 40.1076\n",
            "Epoch [86/500], Loss: 40.1032\n",
            "Epoch [87/500], Loss: 40.1025\n",
            "Epoch [88/500], Loss: 40.1135\n",
            "Epoch [89/500], Loss: 40.1045\n",
            "Epoch [90/500], Loss: 40.1080\n",
            "Epoch [91/500], Loss: 40.1051\n",
            "Epoch [92/500], Loss: 40.0947\n",
            "Epoch [93/500], Loss: 40.0950\n",
            "Epoch [94/500], Loss: 40.0999\n",
            "Epoch [95/500], Loss: 40.1017\n",
            "Epoch [96/500], Loss: 40.1040\n",
            "Epoch [97/500], Loss: 40.0977\n",
            "Epoch [98/500], Loss: 40.0945\n",
            "Epoch [99/500], Loss: 40.0910\n",
            "Epoch [100/500], Loss: 40.0886\n",
            "Epoch [101/500], Loss: 40.0979\n",
            "Epoch [102/500], Loss: 40.0890\n",
            "Epoch [103/500], Loss: 40.0941\n",
            "Epoch [104/500], Loss: 40.0838\n",
            "Epoch [105/500], Loss: 40.0844\n",
            "Epoch [106/500], Loss: 40.0806\n",
            "Epoch [107/500], Loss: 40.0915\n",
            "Epoch [108/500], Loss: 40.0913\n",
            "Epoch [109/500], Loss: 40.0842\n",
            "Epoch [110/500], Loss: 40.0872\n",
            "Epoch [111/500], Loss: 40.0773\n",
            "Epoch [112/500], Loss: 40.0961\n",
            "Epoch [113/500], Loss: 40.0914\n",
            "Epoch [114/500], Loss: 40.0949\n",
            "Epoch [115/500], Loss: 40.0831\n",
            "Epoch [116/500], Loss: 40.0887\n",
            "Epoch [117/500], Loss: 40.0805\n",
            "Epoch [118/500], Loss: 40.0860\n",
            "Epoch [119/500], Loss: 40.0740\n",
            "Epoch [120/500], Loss: 40.0876\n",
            "Epoch [121/500], Loss: 40.0929\n",
            "Epoch [122/500], Loss: 40.0828\n",
            "Epoch [123/500], Loss: 40.0896\n",
            "Epoch [124/500], Loss: 40.0773\n",
            "Epoch [125/500], Loss: 40.0877\n",
            "Epoch [126/500], Loss: 40.0838\n",
            "Epoch [127/500], Loss: 40.0923\n",
            "Epoch [128/500], Loss: 40.0831\n",
            "Epoch [129/500], Loss: 40.0840\n",
            "Epoch [130/500], Loss: 40.0788\n",
            "Epoch [131/500], Loss: 40.0842\n",
            "Epoch [132/500], Loss: 40.0946\n",
            "Epoch [133/500], Loss: 40.0893\n",
            "Epoch [134/500], Loss: 40.0869\n",
            "Epoch [135/500], Loss: 40.0773\n",
            "Epoch [136/500], Loss: 40.0874\n",
            "Epoch [137/500], Loss: 40.0852\n",
            "Epoch [138/500], Loss: 40.0844\n",
            "Epoch [139/500], Loss: 40.0751\n",
            "Epoch [140/500], Loss: 40.0831\n",
            "Epoch [141/500], Loss: 40.0782\n",
            "Epoch [142/500], Loss: 40.0927\n",
            "Epoch [143/500], Loss: 40.0952\n",
            "Epoch [144/500], Loss: 40.0865\n",
            "Epoch [145/500], Loss: 40.0916\n",
            "Epoch [146/500], Loss: 40.0935\n",
            "Epoch [147/500], Loss: 40.0763\n",
            "Epoch [148/500], Loss: 40.0873\n",
            "Epoch [149/500], Loss: 40.0807\n",
            "Epoch [150/500], Loss: 40.0803\n",
            "Epoch [151/500], Loss: 40.0862\n",
            "Epoch [152/500], Loss: 40.0959\n",
            "Epoch [153/500], Loss: 40.0869\n",
            "Epoch [154/500], Loss: 40.0869\n",
            "Epoch [155/500], Loss: 40.0840\n",
            "Epoch [156/500], Loss: 40.0903\n",
            "Epoch [157/500], Loss: 40.0852\n",
            "Epoch [158/500], Loss: 40.0776\n",
            "Epoch [159/500], Loss: 40.0825\n",
            "Epoch [160/500], Loss: 40.0861\n",
            "Epoch [161/500], Loss: 40.0767\n",
            "Epoch [162/500], Loss: 40.0863\n",
            "Epoch [163/500], Loss: 40.0791\n",
            "Epoch [164/500], Loss: 40.0829\n",
            "Epoch [165/500], Loss: 40.0861\n",
            "Epoch [166/500], Loss: 40.0806\n",
            "Epoch [167/500], Loss: 40.0798\n",
            "Epoch [168/500], Loss: 40.0799\n",
            "Epoch [169/500], Loss: 40.0828\n",
            "Epoch [170/500], Loss: 40.0738\n",
            "Epoch [171/500], Loss: 40.0886\n",
            "Epoch [172/500], Loss: 40.0885\n",
            "Epoch [173/500], Loss: 40.0871\n",
            "Epoch [174/500], Loss: 40.0779\n",
            "Epoch [175/500], Loss: 40.0938\n",
            "Epoch [176/500], Loss: 40.0899\n",
            "Epoch [177/500], Loss: 40.0864\n",
            "Epoch [178/500], Loss: 40.0852\n",
            "Epoch [179/500], Loss: 40.0776\n",
            "Epoch [180/500], Loss: 40.0843\n",
            "Epoch [181/500], Loss: 40.0825\n",
            "Epoch [182/500], Loss: 40.0759\n",
            "Epoch [183/500], Loss: 40.0853\n",
            "Epoch [184/500], Loss: 40.0773\n",
            "Epoch [185/500], Loss: 40.0820\n",
            "Epoch [186/500], Loss: 40.0913\n",
            "Epoch [187/500], Loss: 40.0837\n",
            "Epoch [188/500], Loss: 40.0782\n",
            "Epoch [189/500], Loss: 40.0933\n",
            "Epoch [190/500], Loss: 40.0808\n",
            "Epoch [191/500], Loss: 40.0828\n",
            "Epoch [192/500], Loss: 40.0903\n",
            "Epoch [193/500], Loss: 40.0841\n",
            "Epoch [194/500], Loss: 40.0840\n",
            "Epoch [195/500], Loss: 40.0762\n",
            "Epoch [196/500], Loss: 40.0881\n",
            "Epoch [197/500], Loss: 40.0798\n",
            "Epoch [198/500], Loss: 40.0924\n",
            "Epoch [199/500], Loss: 40.0864\n",
            "Epoch [200/500], Loss: 40.0798\n",
            "Epoch [201/500], Loss: 40.0789\n",
            "Epoch [202/500], Loss: 40.0792\n",
            "Epoch [203/500], Loss: 40.0779\n",
            "Epoch [204/500], Loss: 40.0784\n",
            "Epoch [205/500], Loss: 40.0838\n",
            "Epoch [206/500], Loss: 40.0869\n",
            "Epoch [207/500], Loss: 40.0802\n",
            "Epoch [208/500], Loss: 40.0833\n",
            "Epoch [209/500], Loss: 40.0890\n",
            "Epoch [210/500], Loss: 40.0989\n",
            "Epoch [211/500], Loss: 40.0830\n",
            "Epoch [212/500], Loss: 40.0774\n",
            "Epoch [213/500], Loss: 40.0983\n",
            "Epoch [214/500], Loss: 40.0866\n",
            "Epoch [215/500], Loss: 40.0985\n",
            "Epoch [216/500], Loss: 40.0809\n",
            "Epoch [217/500], Loss: 40.0857\n",
            "Epoch [218/500], Loss: 40.0833\n",
            "Epoch [219/500], Loss: 40.0906\n",
            "Epoch [220/500], Loss: 40.0814\n",
            "Epoch [221/500], Loss: 40.0828\n",
            "Epoch [222/500], Loss: 40.0781\n",
            "Epoch [223/500], Loss: 40.0894\n",
            "Epoch [224/500], Loss: 40.0859\n",
            "Epoch [225/500], Loss: 40.0947\n",
            "Epoch [226/500], Loss: 40.0984\n",
            "Epoch [227/500], Loss: 40.0885\n",
            "Epoch [228/500], Loss: 40.0867\n",
            "Epoch [229/500], Loss: 40.0771\n",
            "Epoch [230/500], Loss: 40.0905\n",
            "Epoch [231/500], Loss: 40.0927\n",
            "Epoch [232/500], Loss: 40.0750\n",
            "Epoch [233/500], Loss: 40.0857\n",
            "Epoch [234/500], Loss: 40.0959\n",
            "Epoch [235/500], Loss: 40.0777\n",
            "Epoch [236/500], Loss: 40.0854\n",
            "Epoch [237/500], Loss: 40.0858\n",
            "Epoch [238/500], Loss: 40.1019\n",
            "Epoch [239/500], Loss: 40.0765\n",
            "Epoch [240/500], Loss: 40.0798\n",
            "Epoch [241/500], Loss: 40.0747\n",
            "Epoch [242/500], Loss: 40.0913\n",
            "Epoch [243/500], Loss: 40.0830\n",
            "Epoch [244/500], Loss: 40.0829\n",
            "Epoch [245/500], Loss: 40.0875\n",
            "Epoch [246/500], Loss: 40.0783\n",
            "Epoch [247/500], Loss: 40.0912\n",
            "Epoch [248/500], Loss: 40.0876\n",
            "Epoch [249/500], Loss: 40.0847\n",
            "Epoch [250/500], Loss: 40.0742\n",
            "Epoch [251/500], Loss: 40.0807\n",
            "Epoch [252/500], Loss: 40.0984\n",
            "Epoch [253/500], Loss: 40.0783\n",
            "Epoch [254/500], Loss: 40.0817\n",
            "Epoch [255/500], Loss: 40.0795\n",
            "Epoch [256/500], Loss: 40.0835\n",
            "Epoch [257/500], Loss: 40.0750\n",
            "Epoch [258/500], Loss: 40.0786\n",
            "Epoch [259/500], Loss: 40.0758\n",
            "Epoch [260/500], Loss: 40.0769\n",
            "Epoch [261/500], Loss: 40.0872\n",
            "Epoch [262/500], Loss: 40.0785\n",
            "Epoch [263/500], Loss: 40.0832\n",
            "Epoch [264/500], Loss: 40.0779\n",
            "Epoch [265/500], Loss: 40.0828\n",
            "Epoch [266/500], Loss: 40.0777\n",
            "Epoch [267/500], Loss: 40.0797\n",
            "Epoch [268/500], Loss: 40.0851\n",
            "Epoch [269/500], Loss: 40.0840\n",
            "Epoch [270/500], Loss: 40.0843\n",
            "Epoch [271/500], Loss: 40.0821\n",
            "Epoch [272/500], Loss: 40.0815\n",
            "Epoch [273/500], Loss: 40.0833\n",
            "Epoch [274/500], Loss: 40.0912\n",
            "Epoch [275/500], Loss: 40.0892\n",
            "Epoch [276/500], Loss: 40.0852\n",
            "Epoch [277/500], Loss: 40.0922\n",
            "Epoch [278/500], Loss: 40.0938\n",
            "Epoch [279/500], Loss: 40.0967\n",
            "Epoch [280/500], Loss: 40.0859\n",
            "Epoch [281/500], Loss: 40.0868\n",
            "Epoch [282/500], Loss: 40.0800\n",
            "Epoch [283/500], Loss: 40.0871\n",
            "Epoch [284/500], Loss: 40.0722\n",
            "Epoch [285/500], Loss: 40.0870\n",
            "Epoch [286/500], Loss: 40.0715\n",
            "Epoch [287/500], Loss: 40.0838\n",
            "Epoch [288/500], Loss: 40.0914\n",
            "Epoch [289/500], Loss: 40.0817\n",
            "Epoch [290/500], Loss: 40.0776\n",
            "Epoch [291/500], Loss: 40.0872\n",
            "Epoch [292/500], Loss: 40.0866\n",
            "Epoch [293/500], Loss: 40.0833\n",
            "Epoch [294/500], Loss: 40.0934\n",
            "Epoch [295/500], Loss: 40.0899\n",
            "Epoch [296/500], Loss: 40.0828\n",
            "Epoch [297/500], Loss: 40.0885\n",
            "Epoch [298/500], Loss: 40.0822\n",
            "Epoch [299/500], Loss: 40.0856\n",
            "Epoch [300/500], Loss: 40.0879\n",
            "Epoch [301/500], Loss: 40.0814\n",
            "Epoch [302/500], Loss: 40.0858\n",
            "Epoch [303/500], Loss: 40.0865\n",
            "Epoch [304/500], Loss: 40.0767\n",
            "Epoch [305/500], Loss: 40.0814\n",
            "Epoch [306/500], Loss: 40.0819\n",
            "Epoch [307/500], Loss: 40.0707\n",
            "Epoch [308/500], Loss: 40.0862\n",
            "Epoch [309/500], Loss: 40.0971\n",
            "Epoch [310/500], Loss: 40.0860\n",
            "Epoch [311/500], Loss: 40.0794\n",
            "Epoch [312/500], Loss: 40.0798\n",
            "Epoch [313/500], Loss: 40.0803\n",
            "Epoch [314/500], Loss: 40.0837\n",
            "Epoch [315/500], Loss: 40.0788\n",
            "Epoch [316/500], Loss: 40.0829\n",
            "Epoch [317/500], Loss: 40.0784\n",
            "Epoch [318/500], Loss: 40.0831\n",
            "Epoch [319/500], Loss: 40.0966\n",
            "Epoch [320/500], Loss: 40.0768\n",
            "Epoch [321/500], Loss: 40.0972\n",
            "Epoch [322/500], Loss: 40.0868\n",
            "Epoch [323/500], Loss: 40.0863\n",
            "Epoch [324/500], Loss: 40.0860\n",
            "Epoch [325/500], Loss: 40.0881\n",
            "Epoch [326/500], Loss: 40.0832\n",
            "Epoch [327/500], Loss: 40.0946\n",
            "Epoch [328/500], Loss: 40.0840\n",
            "Epoch [329/500], Loss: 40.0895\n",
            "Epoch [330/500], Loss: 40.0859\n",
            "Epoch [331/500], Loss: 40.0850\n",
            "Epoch [332/500], Loss: 40.0883\n",
            "Epoch [333/500], Loss: 40.0784\n",
            "Epoch [334/500], Loss: 40.0937\n",
            "Epoch [335/500], Loss: 40.0823\n",
            "Epoch [336/500], Loss: 40.0780\n",
            "Epoch [337/500], Loss: 40.0845\n",
            "Epoch [338/500], Loss: 40.0831\n",
            "Epoch [339/500], Loss: 40.0768\n",
            "Epoch [340/500], Loss: 40.0981\n",
            "Epoch [341/500], Loss: 40.0791\n",
            "Epoch [342/500], Loss: 40.0936\n",
            "Epoch [343/500], Loss: 40.0911\n",
            "Epoch [344/500], Loss: 40.0864\n",
            "Epoch [345/500], Loss: 40.0894\n",
            "Epoch [346/500], Loss: 40.0848\n",
            "Epoch [347/500], Loss: 40.0871\n",
            "Epoch [348/500], Loss: 40.0872\n",
            "Epoch [349/500], Loss: 40.0788\n",
            "Epoch [350/500], Loss: 40.0800\n",
            "Epoch [351/500], Loss: 40.0914\n",
            "Epoch [352/500], Loss: 40.0888\n",
            "Epoch [353/500], Loss: 40.0843\n",
            "Epoch [354/500], Loss: 40.0840\n",
            "Epoch [355/500], Loss: 40.0733\n",
            "Epoch [356/500], Loss: 40.0786\n",
            "Epoch [357/500], Loss: 40.0927\n",
            "Epoch [358/500], Loss: 40.0871\n",
            "Epoch [359/500], Loss: 40.0875\n",
            "Epoch [360/500], Loss: 40.0756\n",
            "Epoch [361/500], Loss: 40.0972\n",
            "Epoch [362/500], Loss: 40.0778\n",
            "Epoch [363/500], Loss: 40.0856\n",
            "Epoch [364/500], Loss: 40.0887\n",
            "Epoch [365/500], Loss: 40.0829\n",
            "Epoch [366/500], Loss: 40.0928\n",
            "Epoch [367/500], Loss: 40.0838\n",
            "Epoch [368/500], Loss: 40.0796\n",
            "Epoch [369/500], Loss: 40.0792\n",
            "Epoch [370/500], Loss: 40.0781\n",
            "Epoch [371/500], Loss: 40.0834\n",
            "Epoch [372/500], Loss: 40.0865\n",
            "Epoch [373/500], Loss: 40.0828\n",
            "Epoch [374/500], Loss: 40.0894\n",
            "Epoch [375/500], Loss: 40.0768\n",
            "Epoch [376/500], Loss: 40.0826\n",
            "Epoch [377/500], Loss: 40.0752\n",
            "Epoch [378/500], Loss: 40.0734\n",
            "Epoch [379/500], Loss: 40.0944\n",
            "Epoch [380/500], Loss: 40.0827\n",
            "Epoch [381/500], Loss: 40.0797\n",
            "Epoch [382/500], Loss: 40.0855\n",
            "Epoch [383/500], Loss: 40.0847\n",
            "Epoch [384/500], Loss: 40.0906\n",
            "Epoch [385/500], Loss: 40.0833\n",
            "Epoch [386/500], Loss: 40.0789\n",
            "Epoch [387/500], Loss: 40.0772\n",
            "Epoch [388/500], Loss: 40.0819\n",
            "Epoch [389/500], Loss: 40.0798\n",
            "Epoch [390/500], Loss: 40.0985\n",
            "Epoch [391/500], Loss: 40.0762\n",
            "Epoch [392/500], Loss: 40.0787\n",
            "Epoch [393/500], Loss: 40.0835\n",
            "Epoch [394/500], Loss: 40.0742\n",
            "Epoch [395/500], Loss: 40.0809\n",
            "Epoch [396/500], Loss: 40.0846\n",
            "Epoch [397/500], Loss: 40.0846\n",
            "Epoch [398/500], Loss: 40.0860\n",
            "Epoch [399/500], Loss: 40.0915\n",
            "Epoch [400/500], Loss: 40.0755\n",
            "Epoch [401/500], Loss: 40.0808\n",
            "Epoch [402/500], Loss: 40.0882\n",
            "Epoch [403/500], Loss: 40.0966\n",
            "Epoch [404/500], Loss: 40.0834\n",
            "Epoch [405/500], Loss: 40.0836\n",
            "Epoch [406/500], Loss: 40.0909\n",
            "Epoch [407/500], Loss: 40.0846\n",
            "Epoch [408/500], Loss: 40.0870\n",
            "Epoch [409/500], Loss: 40.0773\n",
            "Epoch [410/500], Loss: 40.0754\n",
            "Epoch [411/500], Loss: 40.0851\n",
            "Epoch [412/500], Loss: 40.0876\n",
            "Epoch [413/500], Loss: 40.0939\n",
            "Epoch [414/500], Loss: 40.0846\n",
            "Epoch [415/500], Loss: 40.0772\n",
            "Epoch [416/500], Loss: 40.0808\n",
            "Epoch [417/500], Loss: 40.0863\n",
            "Epoch [418/500], Loss: 40.0912\n",
            "Epoch [419/500], Loss: 40.0751\n",
            "Epoch [420/500], Loss: 40.0743\n",
            "Epoch [421/500], Loss: 40.0842\n",
            "Epoch [422/500], Loss: 40.0786\n",
            "Epoch [423/500], Loss: 40.0807\n",
            "Epoch [424/500], Loss: 40.0849\n",
            "Epoch [425/500], Loss: 40.0857\n",
            "Epoch [426/500], Loss: 40.0894\n",
            "Epoch [427/500], Loss: 40.0919\n",
            "Epoch [428/500], Loss: 40.0840\n",
            "Epoch [429/500], Loss: 40.0770\n",
            "Epoch [430/500], Loss: 40.0810\n",
            "Epoch [431/500], Loss: 40.0766\n",
            "Epoch [432/500], Loss: 40.0807\n",
            "Epoch [433/500], Loss: 40.0859\n",
            "Epoch [434/500], Loss: 40.0875\n",
            "Epoch [435/500], Loss: 40.0846\n",
            "Epoch [436/500], Loss: 40.0801\n",
            "Epoch [437/500], Loss: 40.0833\n",
            "Epoch [438/500], Loss: 40.0805\n",
            "Epoch [439/500], Loss: 40.0797\n",
            "Epoch [440/500], Loss: 40.0860\n",
            "Epoch [441/500], Loss: 40.0772\n",
            "Epoch [442/500], Loss: 40.0873\n",
            "Epoch [443/500], Loss: 40.0952\n",
            "Epoch [444/500], Loss: 40.0841\n",
            "Epoch [445/500], Loss: 40.0829\n",
            "Epoch [446/500], Loss: 40.0798\n",
            "Epoch [447/500], Loss: 40.0890\n",
            "Epoch [448/500], Loss: 40.0814\n",
            "Epoch [449/500], Loss: 40.0901\n",
            "Epoch [450/500], Loss: 40.0958\n",
            "Epoch [451/500], Loss: 40.0814\n",
            "Epoch [452/500], Loss: 40.0958\n",
            "Epoch [453/500], Loss: 40.0834\n",
            "Epoch [454/500], Loss: 40.0855\n",
            "Epoch [455/500], Loss: 40.0865\n",
            "Epoch [456/500], Loss: 40.0894\n",
            "Epoch [457/500], Loss: 40.0852\n",
            "Epoch [458/500], Loss: 40.0754\n",
            "Epoch [459/500], Loss: 40.0743\n",
            "Epoch [460/500], Loss: 40.0870\n",
            "Epoch [461/500], Loss: 40.0829\n",
            "Epoch [462/500], Loss: 40.0888\n",
            "Epoch [463/500], Loss: 40.0810\n",
            "Epoch [464/500], Loss: 40.0864\n",
            "Epoch [465/500], Loss: 40.0801\n",
            "Epoch [466/500], Loss: 40.0819\n",
            "Epoch [467/500], Loss: 40.0805\n",
            "Epoch [468/500], Loss: 40.0919\n",
            "Epoch [469/500], Loss: 40.0921\n",
            "Epoch [470/500], Loss: 40.0800\n",
            "Epoch [471/500], Loss: 40.0851\n",
            "Epoch [472/500], Loss: 40.0850\n",
            "Epoch [473/500], Loss: 40.0803\n",
            "Epoch [474/500], Loss: 40.0802\n",
            "Epoch [475/500], Loss: 40.0869\n",
            "Epoch [476/500], Loss: 40.0823\n",
            "Epoch [477/500], Loss: 40.0813\n",
            "Epoch [478/500], Loss: 40.0832\n",
            "Epoch [479/500], Loss: 40.0767\n",
            "Epoch [480/500], Loss: 40.0914\n",
            "Epoch [481/500], Loss: 40.0900\n",
            "Epoch [482/500], Loss: 40.0924\n",
            "Epoch [483/500], Loss: 40.0844\n",
            "Epoch [484/500], Loss: 40.1039\n",
            "Epoch [485/500], Loss: 40.0767\n",
            "Epoch [486/500], Loss: 40.0909\n",
            "Epoch [487/500], Loss: 40.0834\n",
            "Epoch [488/500], Loss: 40.0817\n",
            "Epoch [489/500], Loss: 40.0952\n",
            "Epoch [490/500], Loss: 40.0798\n",
            "Epoch [491/500], Loss: 40.0845\n",
            "Epoch [492/500], Loss: 40.0885\n",
            "Epoch [493/500], Loss: 40.0818\n",
            "Epoch [494/500], Loss: 40.0947\n",
            "Epoch [495/500], Loss: 40.0859\n",
            "Epoch [496/500], Loss: 40.1133\n",
            "Epoch [497/500], Loss: 40.0900\n",
            "Epoch [498/500], Loss: 40.0783\n",
            "Epoch [499/500], Loss: 40.0892\n",
            "Epoch [500/500], Loss: 40.0831\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Où sont alors stockées les estimations des  $\\theta_k$ ? Les extraire du réseau *mySimpleNet* dans la cellule suivante."
      ],
      "metadata": {
        "id": "OZwKogEEp2Fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the trained parameters (weights and bias) from the model\n",
        "theta0 = mySimpleNet.fc.bias.item()  # Intercept (θ0)\n",
        "theta1, theta2, theta3 = mySimpleNet.fc.weight[0].detach().numpy()  # Coefficients (θ1, θ2, θ3)\n",
        "\n",
        "# Print the estimated parameters\n",
        "print(\"Estimated coefficients:\")\n",
        "print(f\"θ0 (Intercept): {theta0}\")\n",
        "print(f\"θ1 (x): {theta1}\")\n",
        "print(f\"θ2 (y): {theta2}\")\n",
        "print(f\"θ3 (z): {theta3}\")\n"
      ],
      "metadata": {
        "id": "EjgWp1y1rseb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ce6e2c6-7bae-4eb3-c41c-63ba5bbf701d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated coefficients:\n",
            "θ0 (Intercept): 16.556612014770508\n",
            "θ1 (x): 3.271831512451172\n",
            "θ2 (y): 3.29249906539917\n",
            "θ3 (z): 6.6861724853515625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5** Tester ces estimations sur le jeu de test et comparer avec celles de la question 1. Commentez."
      ],
      "metadata": {
        "id": "pEB-V-oOrJED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract inputs and targets from the test set\n",
        "X_test = test_set['inputs']\n",
        "y_test = test_set['targets']\n",
        "\n",
        "# Linear Regression Predictions\n",
        "y_pred_lr = model.predict(X_test)\n",
        "mse_lr = ((y_pred_lr - y_test) ** 2).mean()\n",
        "\n",
        "# Neural Network Predictions\n",
        "mySimpleNet.eval()  # Set the neural network to evaluation mode\n",
        "with torch.no_grad():  # Disable gradient computation\n",
        "    y_pred_nn = mySimpleNet(torch.tensor(X_test, dtype=torch.float32)).squeeze().numpy()\n",
        "mse_nn = ((y_pred_nn - y_test) ** 2).mean()\n",
        "\n",
        "# Compare the Mean Squared Errors\n",
        "print(f\"Linear Regression Test MSE: {mse_lr:.4f}\")\n",
        "print(f\"Neural Network Test MSE: {mse_nn:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sc8-m-VU2vv4",
        "outputId": "504a15d7-b4d1-4667-b5c0-1ff78e629603"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression Test MSE: 3.9932\n",
            "Neural Network Test MSE: 3.9937\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercice 2** *Champ réceptif et prédiction causale*"
      ],
      "metadata": {
        "id": "CpRvXCaAtsIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le réseau défini dans la cellule suivante est utilisé pour faire le lien entre les valeurs $(x_{t' \\leq t})$ d'une série temporelle d'entrée et la valeur présente $y_t$ d'une série temporelle cible."
      ],
      "metadata": {
        "id": "8JG9wTfK5TBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from utils_exercices import Outconv, Up_causal, Down_causal\n",
        "\n",
        "class Double_conv_causal(nn.Module):\n",
        "    '''(conv => BN => ReLU) * 2, with causal convolutions that preserve input size'''\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=3, dilation=1):\n",
        "        super(Double_conv_causal, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dilation = dilation\n",
        "        self.conv1 = nn.Conv1d(in_ch, out_ch, kernel_size=kernel_size, padding=0, dilation=dilation)\n",
        "        self.bn1 = nn.BatchNorm1d(out_ch)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv1d(out_ch, out_ch, kernel_size=kernel_size, padding=0, dilation=dilation)\n",
        "        self.bn2 = nn.BatchNorm1d(out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.pad(x, ((self.kernel_size - 1) * self.dilation, 0))\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = F.pad(x, ((self.kernel_size - 1) * self.dilation, 0))\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class causalFCN(nn.Module):\n",
        "    def __init__(self, dilation=1):\n",
        "        super(causalFCN, self).__init__()\n",
        "        size = 64\n",
        "        n_channels = 1\n",
        "        n_classes = 1\n",
        "        self.inc = Double_conv_causal(n_channels, size)\n",
        "        self.down1 = Down_causal(size, 2*size)\n",
        "        self.down2 = Down_causal(2*size, 4*size)\n",
        "        self.down3 = Down_causal(4*size, 8*size, pooling_kernel_size=5, pooling_stride=5)\n",
        "        self.down4 = Down_causal(8*size, 4*size, pooling=False, dilation=2)\n",
        "        self.up2 = Up_causal(4*size, 2*size, kernel_size=5, stride=5)\n",
        "        self.up3 = Up_causal(2*size, size)\n",
        "        self.up4 = Up_causal(size, size)\n",
        "        self.outc = Outconv(size, n_classes)\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up2(x5, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        x = self.outc(x)\n",
        "        return x\n",
        "\n",
        "# Exemple d'utilisation\n",
        "model = causalFCN()\n",
        "# Série temporelle d'entrée (x_t):\n",
        "input_tensor1 = torch.rand(1, 1, 10000)\n",
        "# Série temporelle en sortie f(x_t):\n",
        "output = model(input_tensor1)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "fIbU1EJT1MM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** De quel type de réseau de neurones s'agit-il ? Combien de paramètres la couche self.Down1 compte-t-elle (à faire à la main) ?\n",
        "Combien de paramètres le réseau entier compte-t-il (avec un peu de code) ?"
      ],
      "metadata": {
        "id": "-mNnsYU-7R7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Nb de paramètres dans self.Down1: (calcul \"à la main\")\n",
        "\n",
        "params_conv1 = (128 * 64 * 3) + 128  # First Conv1d layer in Double_conv_causal\n",
        "params_conv2 = (128 * 128 * 3) + 128  # Second Conv1d layer in Double_conv_causal\n",
        "params_bn1 = (128 * 2)  # Batch Norm1d (128 weights + 128 biases)\n",
        "params_bn2 = (128 * 2)  # Batch Norm1d (128 weights + 128 biases)\n",
        "params_down1_total = params_conv1 + params_conv2 + params_bn1 + params_bn2\n",
        "print(f\"Nb de paramètres dans self.Down1: {params_down1_total}\")\n",
        "\n",
        "# Nb de paramètres au total:\n",
        "model = causalFCN()\n",
        "total_parameters = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Nb de paramètres au total: {total_parameters}\")"
      ],
      "metadata": {
        "id": "qlYxUf6U9vH1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94303fb0-ce4a-4877-9886-933230a23f90"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nb de paramètres dans self.Down1: 74496\n",
            "Nb de paramètres au total: 2872641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Par quels mécanismes la taille du vecteur d'entrée est-elle réduite ? Comment est-elle restituée dans la deuxième partie du réseau ?"
      ],
      "metadata": {
        "id": "I4D46A0-8LaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Réponse :\n",
        "La taille du vecteur d’entrée est réduite par le biais de max-pooling dans des couches telles que self.down1, qui sous-échantillonnent la résolution temporelle, et grâce à l’augmentation de la profondeur des canaux dans les couches de convolution. Elle est ensuite restaurée dans la seconde partie du réseau à l’aide de convolutions transposées dans des couches comme self.up2, qui effectuent un suréchantillonnage de la résolution temporelle, ainsi que par des connexions de saut (skip connections) qui combinent des caractéristiques haute résolution issues des couches précédentes avec la sortie suréchantillonnée."
      ],
      "metadata": {
        "id": "2v2d-RqT31OD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Par quels mécanismes le champ réceptif est-il augmenté ? Préciser par un calcul la taille du champ réceptif en sortie de *self.inc*."
      ],
      "metadata": {
        "id": "SVNeFnm88yV2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le champ récepteur est augmenté grâce à la dilatation, à la taille du noyau dans les couches de convolution et au max-pooling dans la voie de sous-échantillonnage. Pour self.inc, qui utilise deux couches de convolution avec une taille de noyau de 3 et une dilatation de 1, le champ récepteur augmente comme suit : la première convolution a un champ récepteur de 3, et la seconde convolution l’étend à\n",
        "3+(3−1)=5. Par conséquent, le champ récepteur de self.inc est 5."
      ],
      "metadata": {
        "id": "NzoYNgLC3-ME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Par un bout de code, déterminer empiriquement la taille du champ réceptif associé à la composante $y_{5000}$ du vecteur de sortie. (Indice: considérer les sorties associées à deux inputs qui ne diffèrent que par une composante...)"
      ],
      "metadata": {
        "id": "TVVcBPuA9EP0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "69WMWCSZAg5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q5** $y_{5000}$ dépend-elle des composantes $x_{t, \\space t > 5000}$ ? Justifier de manière empirique puis préciser la partie du code de Double_conv_causal qui garantit cette propriété de \"causalité\" en justifiant.  \n",
        "\n"
      ],
      "metadata": {
        "id": "gZ37skwm-Vpv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PeooRYE-ATGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "---\n",
        "\n",
        "\\"
      ],
      "metadata": {
        "id": "qV52tusgNn6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\\n",
        "\n",
        "Exercice 3: \"Ranknet loss\""
      ],
      "metadata": {
        "id": "bm-sRzmfqc2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un [article récent](https://https://arxiv.org/abs/2403.14144) revient sur les progrès en matière de learning to rank. En voilà un extrait :"
      ],
      "metadata": {
        "id": "Wl8wUjsSM57D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src=\"https://raw.githubusercontent.com/nanopiero/exam_2025/refs/heads/main/utils/png_exercice3.PNG?token=GHSAT0AAAAAAC427DACOPGNDNN6UDOLVLLAZ4BB2JQ\" alt=\"extrait d'un article\" width=\"800\">"
      ],
      "metadata": {
        "id": "SDZUXMlSDpoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q1** Qu'est-ce que les auteurs appellent \"positive samples\" et \"negative samples\" ? Donner un exemple."
      ],
      "metadata": {
        "id": "9NzV1PbMNyuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q2** Dans l'expression de $\\mathcal{L}_{RankNet}$, d'où proviennent les $z_i$ ? Que représentent-ils ?  "
      ],
      "metadata": {
        "id": "yIKQ5Eo9OnPq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3** Pourquoi cette expression conduit-elle à ce que, après apprentissage, \"the estimated\n",
        "value of positive samples is greater than that of negative samples\n",
        "for each pair of positive/negative samples\" ?"
      ],
      "metadata": {
        "id": "r74fWiyvPb7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q4** Dans le cadre d'une approche par deep learning, quels termes utilise-t-on pour qualifier les réseaux de neurones exploités et la modalité suivant laquelle ils sont entraînés ?"
      ],
      "metadata": {
        "id": "pk1EIi_VVi3R"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}